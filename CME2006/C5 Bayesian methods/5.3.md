**Solutions.**

As follows three solutions are provided. The first is the most
complicated in calculation but might be the most straightforward to come
up with, the second much simplifies the first by a smart trick, while
the last one is mainly based on some nice properties of normal
distribution.

**Solution 1.**

Without loss in generality, assume the original normal distribution to
be approximated by MCMC is the standard normal distribution
$Normal(0,1)$. According to the Metropolis-Hastings algorithm, the
acceptance rate for any $\theta$ is

$$A\left( \theta^{'};\theta \right) = \min\left( 1,\frac{\pi\left( \theta^{'} \right)g\left( \theta|\theta^{'} \right)}{\pi(\theta)g\left( \theta^{'}|\theta \right)} \right).$$

Because the proposal distribution $g$ is the same for all $\theta$, the
above can be re-written as

$$\alpha = \min\left( 1,\frac{\pi\left( \theta^{'} \right)}{\pi(\theta)} \right).$$

For any $x$ randomly sampled from the standard normal distribution, its
acceptance rate is equal to the following

$$
\begin{align*}
P &= \iint_{f(y) \geq f(x)}{f(x)q\left( y \middle| x \right)dydx} + \iint_{f(y) < f(x)}{f(x)q\left( y \middle| x \right)\frac{f(y)}{f(x)}dydx} \\
&= \iint_{f(y) \geq f(x)}{f(x)q\left( y \middle| x \right)dydx} + \iint_{f(y) < f(x)}{f(y)q\left( y \middle| x \right)dydx} \\
&= \iint_{f(y) > f(x)}{f(x)q\left( y \middle| x \right)dydx} + \iint_{f(y) < f(x)}{f(y)q\left( y \middle| x \right)dydx} + \iint_{f(y) = f(x)}{f(y)q\left( y \middle| x \right)dydx},
\end{align*}
$$

Define the following

$${I_{1} = \iint_{f(y) > f(x)}^{\ }{f(x)q\left( y \middle| x \right)dydx},}$$

$${I_{2} = \iint_{f(y) < f(x)}^{\ }{f(y)q\left( y \middle| x \right)}dydx,}$$

$${I_{3} = \iint_{f(y) = f(x)}^{\ }{f(y)q\left( y \middle| x \right)}dydx.}$$

Apparently,

$$I_{3} = 0.$$

Hence,

$$\begin{matrix}
P = I_{1} + I_{2} + I_{3} = I_{1} + I_{2}. (5.1) \\
\end{matrix}$$

As to **$I_{2}$**: Because of symmetry of the
density, without loss of generality, consider only $x$ that are
positive. So $I_{2}$ can be written as

$$\begin{align*}
I_{2} &= \iint_{f(y) < f(x)}{q\left( y \middle| x \right)f(y)}dydx \\
&= 2 \times \left( \frac{1}{2\pi\sigma}\int_{0}^{+ \infty}{\int_{x}^{+ \infty}{e^{\frac{- x^{2} - \left( \sigma^{2} + 1 \right)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}} + \frac{1}{2\pi\sigma}\int_{0}^{+ \infty}{\int_{- \infty}^{- x}{e^{\frac{- x^{2} - \left( \sigma^{2} + 1 \right)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}} \right).
\end{align*}
$$

Define

$$I_{21} = \frac{1}{2\pi\sigma}\int_{0}^{+ \infty}{\int_{x}^{+ \infty}{e^{\frac{- x^{2} - \left( \sigma^{2} + 1 \right)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}},$$

$$I_{22} = \frac{1}{2\pi\sigma}\int_{0}^{+ \infty}{\int_{- \infty}^{- x}{e^{\frac{- x^{2} - \left( \sigma^{2} + 1 \right)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}}.$$

We have

$$\begin{matrix}
I_{2} = 2\left( I_{21} + I_{22} \right). (5.2) \\
\end{matrix}$$

**To calculate** $I_{21}$, apply the following linear
transformation

$$\begin{bmatrix}
u \\
v \\
\end{bmatrix} = \begin{bmatrix}
1 & - 1 \\
0 & \sigma \\
\end{bmatrix}\begin{bmatrix}
x \\
y \\
\end{bmatrix},$$

$$(x - y)^{2} + (\sigma y)^{2} = u^{2} + v^{2},$$

and

$$u = x - y,\ v = \sigma y$$

$$x = u + \frac{v}{\sigma},\ y = \frac{v}{\sigma}.$$

Calculate the determinant of the Jacobian as follows

$$|J| = det\begin{bmatrix}
\frac{dx}{du} & \frac{dx}{dv} \\
\frac{dy}{du} & \frac{dy}{dv} \\
\end{bmatrix} = det\begin{bmatrix}
1 & \frac{1}{\sigma} \\
0 & \frac{1}{\sigma} \\
\end{bmatrix} = \frac{1}{\sigma}.$$

For $I_{21}$, the original integral area is the
region enclosed by $x = 0$ and $y = x$, thus $0 < x < y$. It is easy to
see that $0 < u + \frac{1}{\sigma}v < \frac{v}{\sigma},$ thus
$\ 0 > u > - \frac{v}{\sigma}$. This corresponds to the region enclosed
by $u = 0$ and $u = - \frac{1}{\sigma}v$ (thus $v = - \sigma u$). So
$I_{21}$ may be rewritten as

$$\begin{align*}
I_{21} &= \frac{1}{2\pi\sigma}\int_{0}^{+\infty}{\int_{x}^{+\infty}{e^{\frac{-x^{2} - \left(\sigma^{2} + 1\right)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}} \\
&= \frac{1}{2\pi\sigma}\int_{0}^{+\infty}{\int_{-\frac{v}{\sigma}}^{0}{e^{-\frac{u^{2} + v^{2}}{2\sigma^{2}}}|J|dudv}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{\int_{-\frac{v}{\sigma}}^{0}{e^{-\frac{u^{2} + v^{2}}{2\sigma^{2}}}dudv}} \\
&= \int_{0}^{+\infty}{\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{v^{2}}{2\sigma^{2}}}dv\int_{-\frac{v}{\sigma}}^{0}{{\frac{1}{\sqrt{2\pi\sigma^{2}}}e}^{-\frac{u^{2}}{2\sigma^{2}}}du}} \\
&= \frac{\tan^{-1}\left(\frac{1}{\sigma}\right)}{2\pi} \qquad (5.3)
\end{align*}
$$

The last step is because as indicated by the following contour graph of
the probability density of a standard bivariate distribution (left), it
can be seen that
$\int_{0}^{+ \infty}{\frac{1}{\sqrt{(2\pi)\sigma^{2}}}e^{- \frac{v^{2}}{2\sigma^{2}}}dv\int_{- \frac{v}{\sigma}}^{0}{{\frac{1}{\sqrt{(2\pi)\sigma^{2}}}e}^{- \frac{u^{2}}{2\sigma^{2}}}du}}$
is equal to the proportion of
$\frac{\tan^{- 1}\left( \frac{1}{\sigma} \right)}{2\pi}$ of the
probability covered by a standard bivariate normal distribution the
latter of which equals 1.

<p>
 <img src="img/5.3-1.png">
</p>

$$\begin{align*}
I_{21} &= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{\int_{-\frac{v}{\sigma}}^{0}{e^{-\frac{u^{2} + v^{2}}{2\sigma^{2}}}dudv}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{re^{-\frac{r^{2}}{2\sigma^{2}}}dr\int_{\tan^{-1}\left(-\frac{1}{\sigma}\right)}^{0}{d\theta}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{re^{-\frac{r^{2}}{2\sigma^{2}}}dr\int_{0}^{\tan^{-1}\left(\frac{1}{\sigma}\right)}{d\theta}} \\
&= \frac{1}{2\pi\sigma^{2}} \times \frac{\Gamma(1)}{2 \times \frac{1}{2} \times \left(\frac{1}{\sigma}\right)^{2}} \cdot \tan^{-1}\left(\frac{1}{\sigma}\right) \\
&= \frac{\tan^{-1}\left(\frac{1}{\sigma}\right)}{2\pi}.
\end{align*}$$

By solving the above, we get

$$
\tan\beta = - \frac{\sigma}{\sigma^{2} + 2}.
$$

Hence

$$\begin{align*}
I_{22} &= \frac{1}{2\pi\sigma}\int_{0}^{+\infty}{\int_{-\infty}^{-x}{e^{\frac{-x^{2} - (\sigma^{2} + 1)y^{2} + 2\sigma xy}{2\sigma^{2}}}dydx}} \\
&= \frac{1}{2\pi\sigma}\int_{0}^{+\infty}{\int_{-\frac{\sigma}{\sigma^{2} + 2}v}^{0}{e^{-\frac{u^{2} + v^{2}}{2\sigma^{2}}}|J|dudv}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{\int_{-\frac{\sigma}{\sigma^{2} + 2}v}^{0}{e^{-\frac{u^{2} + v^{2}}{2\sigma^{2}}}dudv}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+\infty}{e^{-\frac{(x - y)^{2}}{2\sigma^{2}}}\int_{-\frac{\sigma}{\sigma^{2} + 2}v}^{0}{e^{-\frac{y^{2}}{2\sigma^{2}}}dydx}} \\
&= \frac{\tan^{-1}\left(\frac{\sigma}{\sigma^{2} + 2}\right)}{2\pi}.
\end{align*}$$

Note that the last step in the above can be solved by using the same trick applied in deriving Eq. (5.3). Alternatively, it is also possible to use the transformation from Cartesian coordinates to polar coordinates, so that

$$\begin{align*}
I_{22} &= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+ \infty}{e^{- \frac{r^{2}}{2\sigma^{2}}}rdr\int_{\tan^{- 1}\left( - \frac{\sigma}{\sigma^{2} + 2} \right)}^{0}{d\theta}} \\
&= \frac{1}{2\pi\sigma^{2}}\int_{0}^{+ \infty}{e^{- \frac{r^{2}}{2\sigma^{2}}}rdr\int_{0}^{\tan^{- 1}\left( \frac{\sigma}{\sigma^{2} + 2} \right)}{d\theta}} \\
&= {\frac{1}{2\pi\sigma^{2}} \times \frac{\Gamma(1)}{2 \times \frac{1}{2} \times ({\frac{1}{\sigma})}^{2}} \bullet \tan^{- 1}}\left( \frac{\sigma}{\sigma^{2} + 2} \right) \\
&= \frac{\tan^{- 1}\left( \frac{\sigma}{\sigma^{2} + 2} \right)}{2\pi}.
\end{align*}$$


Further, apply the sum formula for arctangent

$$\arctan\left( A) + arctan(B \right) = \arctan\left( \frac{A + B}{1 - AB} \right).$$

It follows that

$$\begin{align*}
I_{2} &= I_{21} + I_{22} \\
&= 2 \times \left( \frac{\tan^{-1}\left(\frac{1}{\sigma}\right)}{2\pi} + \frac{\tan^{-1}\left(\frac{\sigma}{\sigma^{2} + 2}\right)}{2\pi} \right) \\
&= \frac{1}{\pi}\tan^{-1}\left(\frac{\frac{1}{\sigma} + \frac{\sigma}{\sigma^{2} + 2}}{1 - \frac{1}{\sigma} \cdot \frac{\sigma}{\sigma^{2} + 2}}\right) \\
&= \frac{1}{\pi}\tan^{-1}\left(\frac{2\sigma^{2} + 2}{\sigma^{3} + \sigma}\right)
\end{align*}$$

Similarly, as to $I_{1}$, without loss of generality, considering $x > 0$, $I_{1}$ can be re-written as

$$\begin{align*}
I_{1} &= \iint_{f(y) \geq f(x)}^{\ }{f(x)q\left( y \middle| x \right)dydx} \\
&= 2 \times \frac{1}{\sigma}\iint_{f(y) \geq f(x)}^{\ }{{\frac{1}{\sqrt{2\pi}}e}^{- \frac{x^{2}}{2}}\frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{(x - y)^{2}}{2\sigma^{2}}}dydx} \\
&= \frac{2}{\sigma}\int_{0}^{+ \infty}{\int_{- x}^{x}{{\frac{1}{\sqrt{2\pi}}e}^{- \frac{x^{2}}{2}}\frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{(x - y)^{2}}{2\sigma^{2}}}dydx}} \\
&= \frac{2}{\sigma}\int_{0}^{+ \infty}{{\frac{1}{\sqrt{2\pi}}e}^{- \frac{x^{2}}{2}}dx\int_{- x}^{x}{\frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{(x - y)^{2}}{2\sigma^{2}}}dy}}.
\end{align*}$$

Apply the following linear transformation

$$\begin{bmatrix}
z \\
w \\
\end{bmatrix} = \begin{bmatrix}
\frac{1}{\sigma} & - \frac{1}{\sigma} \\
1 & 0 \\
\end{bmatrix}\begin{bmatrix}
x \\
y \\
\end{bmatrix},$$

so that

$$x = w + \sigma z,y = w.$$

The determinant of the Jacobian is calculated as

$$|J| = \det\begin{bmatrix}
\frac{dx}{dz} & \frac{dx}{dw} \\
\frac{dy}{dz} & \frac{dy}{dw} \\
\end{bmatrix} = \left| \begin{matrix}
\sigma & 1 \\
0 & 1 \\
\end{matrix} \right| = \sigma.$$

The above can thus be re-written as

$$\begin{align*}
I_{1} &= \frac{2}{\sigma}\int_{0}^{+\infty}{{\frac{1}{\sqrt{2\pi}}e}^{-\frac{w^{2}}{2}}dw\int_{-x}^{x}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{z^{2}}{2\sigma^{2}}}|J|dz}} \\
&= \sigma \cdot \frac{2}{\sigma} \cdot \int_{0}^{+\infty}{{\frac{1}{\sqrt{2\pi}}e}^{-\frac{w^{2}}{2}}dw\int_{0}^{\frac{2w}{\sigma}}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{z^{2}}{2}}dz}} \\
&= 2\int_{0}^{+\infty}{{\frac{1}{\sqrt{2\pi}}e}^{-\frac{w^{2}}{2}}dw\int_{0}^{\frac{2w}{\sigma}}{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{z^{2}}{2}}dz}} \\
&= \frac{1}{\pi}\tan^{-1}\left(\frac{2}{\sigma}\right). (5.5)
\end{align*}$$

Finally, by plugging Eqs. (5.3-5.5) into Eq. (5.1), we have
$$P_{jump} = I_{1} + I_{2} = \frac{2}{\pi}\tan^{- 1}\left( \frac{2}{\sigma} \right).$$

    
**Solution 2.**

In Solution 1 we see that $I_{1} = I_{2}$ and that calculating $I_{1}$
is easier than $I_{2}$. As follows, it can be shown that $I_{1} = I_{2}$
is not a coincidence, and as such, it implies that we can greatly
simplify the calculation by computing only $I_{1}$ ​and then multiplying
it by two to obtain $P$.

$$\begin{align*}
I_{1} &= \iint_{f(y) > f(x)}^{\ }{f(x)q\left( y \middle| x \right)dydx} \\
&= \iint_{|y| > |x|}^{\ }{f(x)q\left( y \middle| x \right)dydx} \\
&= \iint_{|x| > |y|}^{\ }{f(y)q\left( x \middle| y \right)dxdy} \quad \text{(swapping } x \text{ and } y) \\
&= \iint_{|x| > |y|}^{\ }{f(y)q\left( x \middle| y \right)dydx} \\
&= I_{2}.
\end{align*}$$

The following steps are the same as Solution 1, but there will not be
any need to calculate $I_{2}$ which is more difficult to calculate than
$I_{1}$ any more.


    
**Solution 3.**

Define

$$X\sim Normal(0,1),$$

$$Y\sim Normal(X,\theta).$$

Further, define

$$f_{XY}(X = x,Y = y) = \pi(x)q\left( y \middle| x \right).$$

According to the above, we have

$$
\begin{align*}
P_{\text{jump}} &= E\left( \alpha(X,Y) \right) \\
&= E\left( \min\left( 1,\frac{f_{XY}(X = y,Y = x)}{f_{XY}(X = x,Y = y)} \right) \right) \\
&= 2 \times P\left( f_{XY}(X,Y) < f_{XY}(X,Y) \right).
\end{align*}
$$

It can also be shown that

$${f_{XY}(X,Y) = f_{X}(X)f_{Y}\left( Y|X \right)
}{= \frac{1}{\sqrt{2\pi}}e^{- \frac{(X - \theta)^{2}}{2}} \times \frac{1}{\sqrt{2\pi}}e^{- \frac{(Y - X)^{2}}{2\sigma^{2}}}.}$$

Likewise,

$$f_{XY}(Y,X) = \frac{1}{\sqrt{2\pi}}e^{- \frac{(Y - \theta)^{2}}{2}} \times \frac{1}{\sqrt{2\pi}}e^{- \frac{(X - Y)^{2}}{2\sigma^{2}}}.$$

Hence,

$$
\begin{align*}
P_{jump} &= 2P\left( \frac{1}{\sqrt{2\pi}}e^{- \frac{(X - \theta)^{2}}{2}}\frac{1}{\sqrt{2\pi}}e^{- \frac{(Y - X)^{2}}{2\sigma^{2}}} < \frac{1}{\sqrt{2\pi}}e^{- \frac{(Y - \theta)^{2}}{2}}\frac{1}{\sqrt{2\pi}}e^{- \frac{(X - Y)^{2}}{2\sigma^{2}}} \right) \\
&= 2P\left( - \frac{(X - \theta)^{2}}{2} - \frac{(Y - X)^{2}}{2\sigma^{2}} < - \frac{(Y - \theta)^{2}}{2} - \frac{(X - Y)^{2}}{2\sigma^{2}} \right) \\
&= 2P\left( (X - \theta)^{2} > (Y - \theta)^{2} \right) \\
&= 2P\left( (X - Y)(X + Y - 2\theta) > 0 \right).
\end{align*}
$$

According to the context, we have

$$X - 1\sim Normal(0,1),$$

$$Y - X\sim Normal\left( 0,\sigma^{2} \right).$$

Also, $X$ and $Y$ are independent variables. Apply the following
transformation

$$
\begin{cases}
U = X - \theta \\
V = \frac{Y - X}{\sigma} \\
\end{cases}
$$

Equivalently,

$$
\begin{cases}
X = U + \theta \\
Y = \sigma V + U + \theta \\
\end{cases}.
$$

So

$$
\begin{align*}
P_{jump} &= 2P\left( - \sigma V(\sigma V + 2U) > 0 \right) \\
&= 2P\left( V(\sigma V + 2U) < 0 \right) \\
&= 2\left( P(V < 0,\sigma V + 2U > 0) + P(V > 0,\sigma V + 2U < 0) \right) \\
&= 2\left( P\left( V < 0,U > - \frac{\sigma V}{2} \right) + P\left( V > 0,U < - \frac{\sigma V}{2} \right) \right).
\end{align*}
$$

Using corresponding results from Solution 1, it is clear that

$$P\left( V < 0,U > - \frac{\sigma V}{2} \right) = P\left( V > 0,U < - \frac{\sigma V}{2} \right) = \frac{\tan^{- 1}\left( \frac{2}{\sigma} \right)}{2\pi}.$$

Thus

$$P_{jump}\mathbf{=}2\mathbf{\times}2 \times \frac{\tan^{- 1}\left( \frac{2}{\sigma} \right)}{2\pi}\mathbf{=}\frac{2}{\pi}\tan^{- 1}\left( \frac{2}{\sigma} \right).$$
